# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AgQGLsvwunYi3MLV7l1bg8U2G2DpuaWF
"""

# app.py — Streamlit PhysioNet single-record inference (AE + CNN, multi-lead pipeline)

import streamlit as st
import os
import numpy as np
import wfdb
from wfdb import processing
from scipy.signal import butter, filtfilt, resample_poly
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import glob
import random

# --- CONFIG ---
CONFIG = {
    "WORKDIR": "/content/ecg_project",
    "ZIP_PATH": "/content/drive/MyDrive/mitdb.zip", # This might not be used in the app, but keeping for consistency
    "PHYSIONET_DOWNLOAD": True, # Set to True to download directly in the app
    "TARGET_FS": 250,
    "BANDPASS": (0.5, 30.0),
    "WIN_SEC": 2.0,
    "VAL_FRACTION": 0.15, # Not used in the app
    "TEST_FRACTION": 0.15, # Not used in the app
    "RANDOM_SEED": 1337,
    "EPOCHS_AE": 20, # Not used in the app
    "EPOCHS_CNN": 20, # Not used in the app
    "BATCH": 128, # Not used in the app
    "NUM_LEADS": 2, # Adjusted for 2-lead processing
    "PAD_VALUE": 0.0,
    "LEAD_PRIORITY": ["MLII","V5","V1","V2","V3","V4","V6","I","II","III","aVR","aVL","aVF"]
}

# --- Initial Setup ---
os.makedirs(CONFIG["WORKDIR"], exist_ok=True)
random.seed(CONFIG["RANDOM_SEED"])
np.random.seed(CONFIG["RANDOM_SEED"])
tf.random.set_seed(CONFIG["RANDOM_SEED"])

# Define DATA_DIR
DATA_DIR = f'{CONFIG["WORKDIR"]}/mitdb'
os.makedirs(DATA_DIR, exist_ok=True)

# Define the path for models
MODEL_DIR = "./models"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# # app.py — Streamlit PhysioNet single-record inference (AE + CNN, multi-lead pipeline)
# 
# import os
# import time
# import random
# import math
# import numpy as np
# import streamlit as st
# import matplotlib.pyplot as plt
# 
# # libs used in training
# import wfdb
# from scipy.signal import butter, filtfilt, resample_poly
# from tensorflow import keras
# 
# # -------------------------
# # CONFIG (edit if needed)
# # -------------------------
# CONFIG = {
#     "WORKDIR": "./physionet_data",
#     "MODELS_DIR": "./models",                     # put autoencoder.keras and cnn_2lead.keras here
#     "AUTOENC_PATH": "./models/autoencoder.keras",
#     "CLF_PATH": "./models/cnn_2lead.keras",
#     "AE_THRESHOLD_PATH": "./models/ae_threshold.npy", # Path to save/load AE threshold
# 
#     "TARGET_FS": 250, # Match the sampling frequency used during training
#     "BANDPASS": (0.5, 30.0), # Match the bandpass filter used during training
#     "WIN_SEC": 2.0, # Match the window size used during training
#     "FRAME_LEN": int(2.0 * 250), # Window size in samples (WIN_SEC * TARGET_FS)
#     # Note: Frame length and stride are less relevant here as we are segmenting based on annotations/R-peaks
#     # However, keep them consistent with training if possible or adapt segmentation logic.
#     # Stride is not used in the original load_record_segments_multi, segmenting is annotation-based.
# 
#     "NUM_LEADS": 3, # Match the number of leads used during training
#     "PAD_VALUE": 0.0, # Match the padding value used during training
#     "LEAD_PRIORITY": ["MLII","V5","V1","V2","V3","V4","V6","I","II","III","aVR","aVL","aVF"], # Match lead priority
# 
#     "SEED": 42,
#     "EPS_STD": 1e-8,
# }
# 
# random.seed(CONFIG["SEED"])
# np.random.seed(CONFIG["SEED"])
# 
# # Ensure work directory and model directory exist
# os.makedirs(CONFIG["WORKDIR"], exist_ok=True)
# os.makedirs(CONFIG["MODELS_DIR"], exist_ok=True)
# 
# 
# # -------------------------
# # Data Loading and Preprocessing Functions (Adapted from Notebook)
# # -------------------------
# 
# # Safer bandpass() - adapted from notebook fix
# def bandpass(sig, fs, lo, hi, order=4):
#     nyq = fs/2.0
#     lo_n = max(1e-6, float(lo)/nyq)
#     hi_n = min(0.999, float(hi)/nyq)
#     if not (0 < lo_n < hi_n < 1):
#         # fallback: clamp to a safe band
#         lo_n, hi_n = 0.004, 0.499
#     b, a = butter(order, [lo_n, hi_n], btype='band')
#     return filtfilt(b, a, sig, method="gust")
# 
# # Preprocessing for a single lead - adapted from notebook fix
# def preprocess_single_lead(x, fs_src, fs_tgt, bp):
#     """
#     Resample and bandpass filter a single lead signal.
# 
#     Args:
#         x (np.ndarray): Input signal for a single lead.
#         fs_src (int): Source sampling frequency.
#         fs_tgt (int): Target sampling frequency.
#         bp (tuple): Bandpass filter frequencies (lo, hi).
# 
#     Returns:
#         np.ndarray: Processed signal.
#     """
#     # Ensure scipys resample_poly is available
#     # In app.py, we assume SciPy is installed as a dependency
#     # if not SCIPY_OK:
#     #     st.error("SciPy not available. Cannot resample signal. Please ensure SciPy is installed.")
#     #     return x # Return original signal if resampling is not possible
# 
#     # Resample
#     from math import gcd # Import gcd here for local scope
#     up, dn = fs_tgt, int(fs_src)
#     g = gcd(up, dn)
#     up //= g
#     dn //= g
#     x_rs = resample_poly(x, up, dn)
# 
#     # Bandpass filter
#     x_bp = bandpass(x_rs, fs_tgt, bp[0], bp[1])
# 
#     # Standardize (handle potential zero std)
#     mean_x = np.mean(x_bp)
#     std_x = np.std(x_bp)
#     if std_x < CONFIG["EPS_STD"]:
#         return x_bp - mean_x # Just remove mean if std is too small
#     return (x_bp - mean_x) / std_x
# 
# # Helper to pick lead indices based on priority
# def pick_lead_indices(sig_names, wanted, k):
#     idx = []
#     for name in wanted:
#         if name in sig_names and len(idx) < k:
#             idx.append(sig_names.index(name))
#     # Fill up remaining indices if needed from available leads
#     for i, _ in enumerate(sig_names):
#         if len(idx) >= k:
#             break
#         if i not in idx:
#             idx.append(i)
#     return idx[:k]
# 
# # Load record segments - adapted from notebook, including padding fix
# def load_record_segments_multi(path_noext, fs_tgt=250, win_sec=2.0, mode="binary"):
#     """
#     Loads a PhysioNet record, preprocesses selected leads, and segments it
#     around annotations or detected R-peaks. Handles padding.
# 
#     Args:
#         path_noext (str): Path to the record file without extension.
#         fs_tgt (int): Target sampling frequency.
#         win_sec (float): Window size in seconds.
#         mode (str): Classification mode (used for mapping symbols).
# 
#     Returns:
#         tuple: (segments, labels) - numpy arrays of shape (N, segment_len, num_leads)
#                and (N,), where N is the number of segments. Returns empty arrays
#                if no segments are found.
#     """
#     try:
#         rec = wfdb.rdrecord(path_noext)
#         fs_src = rec.fs
#         sig = rec.p_signal.astype(np.float64)
#         sig_names = list(rec.sig_name)
#     except Exception as e:
#         st.error(f"Error reading record {os.path.basename(path_noext)}: {e}")
#         return np.empty((0, int(win_sec * fs_tgt), CONFIG["NUM_LEADS"])), np.empty((0,))
# 
#     K = CONFIG["NUM_LEADS"]
#     # Select leads based on priority, handling cases with fewer leads
#     use_idx = pick_lead_indices(sig_names, CONFIG["LEAD_PRIORITY"], min(K, sig.shape[1]))
# 
#     if not use_idx:
#         st.warning(f"Could not find any leads from priority list in record {os.path.basename(path_noext)}.")
#         return np.empty((0, int(win_sec * fs_tgt), K)), np.empty((0,))
# 
#     # Preprocess chosen leads
#     chosen = [preprocess_single_lead(sig[:, i], fs_src, fs_tgt, CONFIG["BANDPASS"]) for i in use_idx]
# 
#     # Stack preprocessed leads
#     Xstack = np.stack(chosen, axis=0)
# 
#     # Pad with zeros if fewer than K leads were available
#     if len(use_idx) < K:
#         pad_shape = (K - len(use_idx), Xstack.shape[1])
#         pad = np.full(pad_shape, CONFIG["PAD_VALUE"], dtype=Xstack.dtype)
#         Xstack = np.concatenate([Xstack, pad], axis=0)
# 
#     Xstack = Xstack.T  # Shape becomes (T, K)
# 
#     # Get annotations or detect R-peaks
#     try:
#         ann = wfdb.rdann(path_noext, 'atr')
#         # Adjust annotation samples to target frequency
#         annsamp = (ann.sample * (fs_tgt / fs_src)).astype(int)
#         anntype = ann.symbol
#         # st.write(f"Record {os.path.basename(path_noext)}: Loaded {len(annsamp)} annotations.") # Optional: verbose logging
#     except Exception as e:
#         # st.warning(f"Could not load annotations for record {os.path.basename(path_noext)}. Attempting R-peak detection. Error: {e}") # Optional: verbose logging
#         try:
#             # Use gqrs_detect on the first available lead for R-peak detection
#             annsamp = processing.gqrs_detect(sig=Xstack[:, 0], fs=fs_tgt)
#             anntype = ['N'] * len(annsamp) # Assume all detected peaks are normal
#             # st.write(f"Record {os.path.basename(path_noext)}: Detected {len(annsamp)} R-peaks.") # Optional: verbose logging
#         except Exception as e_gqrs:
#             st.error(f"Failed to detect R-peaks for record {os.path.basename(path_noext)}. Error: {e_gqrs}")
#             return np.empty((0, int(win_sec * fs_tgt), K)), np.empty((0,))
# 
# 
#     half = int(win_sec * fs_tgt / 2)
#     segment_len = 2 * half
#     Ttot = Xstack.shape[0]
# 
#     Xs, ys = [], []
#     # Extract segments around annotation/R-peak samples
#     for s, sy in zip(annsamp, anntype):
#         # Calculate the start and end indices for the segment
#         l = s - half
#         r = s + half
# 
#         # Calculate required padding
#         pad_l = max(0, -l)
#         pad_r = max(0, r - Ttot)
# 
#         # Adjust start and end indices based on padding
#         l_padded = l + pad_l
#         r_padded = r + pad_r
# 
#         # Pad the signal array if necessary
#         if pad_l > 0 or pad_r > 0:
#             padded_Xstack = np.pad(Xstack, ((pad_l, pad_r), (0, 0)), mode='constant', constant_values=CONFIG["PAD_VALUE"])
#         else:
#             padded_Xstack = Xstack
# 
#         # Extract the segment from the padded array
#         # Ensure segment length is correct after padding adjustment
#         seg = padded_Xstack[l_padded : l_padded + segment_len, :]
# 
# 
#         # Check if the extracted segment has the correct length (should be guaranteed by padding logic now)
#         if seg.shape[0] == segment_len:
#              # Check for NaN/Inf in the segment after processing
#             if np.any(np.isnan(seg)) or np.any(np.isinf(seg)):
#                 # Optionally handle or skip segments with invalid values
#                 # st.warning(f"Segment at sample {s} contains NaN or Inf values. Skipping.") # Optional: verbose logging
#                 continue
#             Xs.append(seg.astype(np.float32))
#             # Map symbol to label (assuming binary mode 0=Normal, 1=Abnormal)
#             if mode == "binary":
#                  # Need map_symbol function for binary mapping
#                  NORMAL_SYMS=set(['N','L','R','e','j']); PVC_SYMS=set(['V','E']) # Define here or import
#                  ys.append(0 if sy in NORMAL_SYMS else 1)
#             else:
#                  # Handle other modes if needed
#                  ys.append(-1) # Placeholder for other modes
# 
# 
#     # st.write(f"Record {os.path.basename(path_noext)}: Found {len(Xs)} valid segments.") # Optional: verbose logging
# 
#     if not Xs:
#         return np.empty((0, segment_len, K)), np.empty((0,))
# 
#     # Note: For prediction in app, we might not need the ground truth labels 'ys'
#     # but we return them for consistency if available (from annotations)
#     return np.stack(Xs), np.asarray(ys)
# 
# 
# # Helper function for mapping symbols (copied from notebook)
# NORMAL_SYMS=set(['N','L','R','e','j']); PVC_SYMS=set(['V','E'])
# APB_SYMS=set(['A','a','J','S']); FUSION_SYMS=set(['F'])
# def map_symbol(s, mode="binary"):
#     if mode=="binary": return 0 if s in NORMAL_SYMS else 1
#     if s in NORMAL_SYMS: return 0
#     if s in PVC_SYMS: return 1
#     if s in APB_SYMS: return 2
#     if s in FUSION_SYMS: return 3
#     return 4
# 
# # Helper to check if a channel is good (copied from notebook)
# def is_channel_good(x, eps=1e-8):
#     return np.all(np.isfinite(x)) and (np.std(x) > eps)
# 
# # Helper to find the pair with lowest correlation (copied from notebook)
# def pair_with_lowest_corr(leads_2d):
#     # leads_2d: (L, T)
#     L, T = leads_2d.shape
#     best = (0, 1, 2.0) # Initialize with a high correlation value
#     for i in range(L):
#         for j in range(i+1, L):
#             xi, xj = leads_2d[i], leads_2d[j]
#             # Ensure both channels are 'good' before calculating correlation
#             if is_channel_good(xi) and is_channel_good(xj):
#                 r = np.corrcoef(xi, xj)[0, 1]
#                 a = abs(r) if np.isfinite(r) else 2.0 # Use absolute correlation, handle NaN/Inf from corrcoef
#             else:
#                 a = 2.0 # Assign high correlation if channels are not good
#             if a < best[2]:
#                 best = (i, j, a)
#     return best # Returns indices within the provided leads_2d, not original signal indices
# 
# 
# # Function to select the best 2 leads based on lowest correlation
# # This function is not strictly needed if load_record_segments_multi is adapted
# # to do lead selection based on priority and CONFIG["NUM_LEADS"].
# # If you want a specific "best 2 leads" *model* vs a multi-lead *model* with padding,
# # you would need to adjust which model you load and how data is prepared.
# # Let's stick to the multi-lead approach with padding as used in training for this app.
# # If you specifically need a "best 2 leads" selection before padding for a 2-lead model,
# # the load_record_segments_multi function would need significant modification or a new function created.
# # For consistency with the multi-lead training notebook, the load_record_segments_multi
# # handles lead selection based on CONFIG["LEADS_PRIORITY"] and CONFIG["NUM_LEADS"] with padding.
# 
# 
# # -------------------------
# # Streamlit App
# # -------------------------
# 
# st.title("ECG Anomaly Detection and Classification (AE + CNN)")
# st.write("Test PhysioNet records using trained Autoencoder and CNN models.")
# 
# # --- Load Models ---
# # Use Streamlit caching to avoid reloading models on every interaction
# @st.cache_resource
# def load_models(ae_path, clf_path):
#     ae_model = None
#     clf_model = None
# 
#     if not os.path.exists(ae_path):
#          st.warning(f"Autoencoder model file not found at {ae_path}. Please train and save the model.")
#     else:
#         try:
#             ae_model = keras.models.load_model(ae_path)
#             st.success(f"Autoencoder model loaded successfully from {ae_path}")
#         except Exception as e:
#             st.error(f"Error loading Autoencoder model from {ae_path}: {e}")
# 
#     if not os.path.exists(clf_path):
#          st.warning(f"CNN classifier model file not found at {clf_path}. Please train and save the model.")
#     else:
#         try:
#             clf_model = keras.models.load_model(clf_path)
#             st.success(f"CNN classifier model loaded successfully from {clf_path}")
#         except Exception as e:
#             st.error(f"Error loading CNN classifier model from {clf_path}: {e}")
# 
# 
#     return ae_model, clf_model
# 
# # Load the models when the app starts
# ae_model, clf_model = load_models(CONFIG["AUTOENC_PATH"], CONFIG["CLF_PATH"])
# 
# # --- Load AE Threshold ---
# # Use Streamlit caching for the threshold
# @st.cache_data
# def load_ae_threshold(threshold_path):
#     if not os.path.exists(threshold_path):
#          st.warning(f"AE threshold file not found at {threshold_path}. Anomaly detection will not work.")
#          st.warning("Please run the notebook to train the autoencoder and save the threshold.")
#          return None
#     else:
#         try:
#             threshold = np.load(threshold_path)
#             st.success(f"AE threshold loaded successfully from {threshold_path}")
#             return float(threshold)
#         except Exception as e:
#             st.error(f"Error loading AE threshold from {threshold_path}: {e}")
#             return None
# 
# ae_threshold = load_ae_threshold(CONFIG["AE_THRESHOLD_PATH"])
# 
# # --- User Input ---
# st.sidebar.header("Settings")
# record_id_input = st.sidebar.text_input("Enter PhysioNet Record ID (e.g., 100)", "100")
# # You can add more settings here, e.g., number of segments to plot
# 
# # --- Prediction ---
# if st.button("Analyze Record"):
#     if ae_model is None or clf_model is None:
#         st.error("Models are not loaded. Please ensure the model files exist in the 'models' directory and were loaded successfully.")
#     elif ae_threshold is None:
#          st.error("AE threshold is not loaded. Anomaly detection cannot be performed.")
#     else:
#         st.info(f"Analyzing record: {record_id_input}...")
#         start_time = time.time()
# 
#         DATA_DIR = f'{CONFIG["WORKDIR"]}/mitdb'
#         record_path_noext = os.path.join(DATA_DIR, record_id_input)
# 
#         # 1. Download record if not exists
#         # Need to handle potential errors during download
#         try:
#             if not os.path.exists(f"{record_path_noext}.hea"):
#                 st.write(f"Record {record_id_input} not found locally. Attempting download from PhysioNet...")
#                 # Use raise_overwrite_error=False to avoid errors if file partially exists
#                 wfdb.dl_database('mitdb', dl_dir=DATA_DIR, records=[record_id_input], raise_overwrite_error=False)
#                 st.success(f"Downloaded record {record_id_input}.")
#             else:
#                  st.write(f"Record {record_id_input} found locally.")
#         except Exception as e:
#             st.error(f"Error downloading record {record_id_input} from PhysioNet: {e}")
#             st.stop() # Stop execution if download fails
# 
# 
#         # 2. Load and preprocess segments
#         # load_record_segments_multi will handle lead selection based on CONFIG["NUM_LEADS"]
#         # and padding for records with fewer leads, matching the training setup.
#         # If you have a specific 'best 2 leads' model, you'd need to adapt this loading.
#         # For consistency with the notebook's training (using CONFIG["NUM_LEADS"] and padding),
#         # we use the multi-lead loading here.
# 
#         X_record, y_record_true = load_record_segments_multi(record_path_noext,
#                                                               fs_tgt=CONFIG["TARGET_FS"],
#                                                               win_sec=CONFIG["WIN_SEC"],
#                                                               mode="binary") # Assuming binary mode for prediction
# 
# 
#         if X_record.size == 0:
#             st.warning(f"No valid segments found for record {record_id_input}. Cannot perform analysis.")
#         else:
#             # 3. Perform Predictions
#             st.write(f"Found {len(X_record)} segments. Performing predictions...")
# 
#             # Autoencoder prediction (anomaly detection)
#             # Ensure ae_model is not None before predicting
#             if ae_model:
#                 ae_rec = ae_model.predict(X_record, verbose=0)
#                 ae_err = np.mean((ae_rec - X_record)**2, axis=(1, 2))
# 
#                 # Anomaly prediction based on threshold (ensure threshold is loaded)
#                 if ae_threshold is not None:
#                      ae_pred_anomalies = (ae_err > ae_threshold).astype(int)
#                      ae_anomaly_rate = ae_pred_anomalies.mean()
#                 else:
#                      ae_anomaly_rate = None
#                      st.warning("AE threshold not loaded. Skipping AE anomaly detection.")
#             else:
#                  ae_anomaly_rate = None
#                  st.warning("AE model not loaded. Skipping AE anomaly detection.")
# 
# 
#             # CNN classifier prediction
#             # Ensure clf_model is not None before predicting
#             if clf_model:
#                 cnn_log = clf_model.predict(X_record, verbose=0)
#                 cnn_pred_classes = np.argmax(cnn_log, axis=1)
#                 cnn_abnormal_rate = (cnn_pred_classes == 1).mean() # Assuming 1 is the abnormal class
#             else:
#                  cnn_abnormal_rate = None
#                  st.warning("CNN model not loaded. Skipping CNN classification.")
# 
# 
#             end_time = time.time()
# 
#             # 4. Display Results
#             st.subheader("Analysis Results")
#             st.write(f"Record ID: **{record_id_input}**")
#             st.write(f"Total Segments Analyzed: **{len(X_record)}**")
#             st.write(f"Analysis Time: **{end_time - start_time:.2f} seconds**")
# 
#             st.subheader("Autoencoder Anomaly Detection")
#             if ae_anomaly_rate is not None:
#                  st.write(f"Proportion of segments detected as anomalies (AE Error > {ae_threshold:.4f}): **{ae_anomaly_rate:.3f}**")
#             else:
#                  st.write("AE Anomaly Detection skipped due to missing model or threshold.")
# 
# 
#             st.subheader("CNN Classification")
#             if cnn_abnormal_rate is not None:
#                  st.write(f"Proportion of segments classified as abnormal: **{cnn_abnormal_rate:.3f}**")
#             else:
#                  st.write("CNN Classification skipped due to missing model.")
# 
# 
#             # Display sample segments with predictions
#             st.subheader("Sample Segment Predictions")
#             num_plots = min(4, len(X_record))
#             if num_plots > 0 and ae_model and clf_model and ae_threshold is not None:
#                 sample_indices = np.random.choice(len(X_record), num_plots, replace=False)
#                 # Use FRAME_LEN which is segment_len calculated as 2*half
#                 segment_len = int(CONFIG["WIN_SEC"] * CONFIG["TARGET_FS"])
#                 t = np.linspace(-CONFIG["WIN_SEC"]/2, CONFIG["WIN_SEC"]/2, segment_len)
#                 for i, j in enumerate(sample_indices):
#                     fig, ax = plt.subplots(figsize=(10, 3))
#                     for ch in range(X_record.shape[2]):
#                         # Offset leads for visibility
#                         ax.plot(t, X_record[j, :, ch] + 3 * ch)
#                     ax.set_title(f"Segment {j+1} | AE Error={ae_err[j]:.4f} (Anomaly: {ae_pred_anomalies[j]}) | CNN Pred Class: {cnn_pred_classes[j]}")
#                     ax.set_xlabel("Time (s)")
#                     ax.set_ylabel("Amplitude (Offset for Leads)")
#                     st.pyplot(fig)
#                     plt.close(fig) # Close the figure to free memory
#             else:
#                 if num_plots == 0:
#                     st.write("No segments to plot.")
#                 else:
#                     st.write("Skipping sample segment plots due to missing models or threshold.")
# 
# 
# # --- Instructions for Running ---
# st.sidebar.subheader("How to Run")
# st.sidebar.write("1. Ensure you have `streamlit`, `wfdb`, `numpy`, `scipy`, and `tensorflow` installed (`pip install streamlit wfdb numpy scipy tensorflow`).")
# st.sidebar.write("2. Save the trained models (`autoencoder.keras`, `cnn_2lead.keras`) and the AE threshold (`ae_threshold.npy`) into a folder named `models` in the same directory as this script.")
# st.sidebar.write("3. Save this code as `app.py`.")
# st.sidebar.write("4. Open your terminal or command prompt.")
# st.sidebar.write(f"5. Navigate to the directory where you saved `app.py` and the `models` folder.")
# st.sidebar.write("6. Run the command: `streamlit run app.py`")
# st.sidebar.write("7. The app will open in your web browser.")
# 
# st.sidebar.subheader("Note on Trained Models")
# st.sidebar.write(f"This app assumes the models were trained with {CONFIG['NUM_LEADS']} leads (padded if necessary), a target frequency of {CONFIG['TARGET_FS']} Hz, and a window size of {CONFIG['WIN_SEC']} seconds.")
# st.sidebar.write("Ensure your saved models and threshold match these parameters.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import os
# import numpy as np
# import wfdb
# from wfdb import processing
# from scipy.signal import butter, filtfilt, resample_poly
# import tensorflow as tf
# from tensorflow import keras
# import matplotlib.pyplot as plt
# import glob
# import random
# from math import gcd
# 
# 
# # --- CONFIG ---
# CONFIG = {
#     "WORKDIR": "/content/ecg_project",
#     "ZIP_PATH": "/content/drive/MyDrive/mitdb.zip", # This might not be used in the app, but keeping for consistency
#     "PHYSIONET_DOWNLOAD": True, # Set to True to download directly in the app
#     "TARGET_FS": 250,
#     "BANDPASS": (0.5, 30.0),
#     "WIN_SEC": 2.0,
#     "VAL_FRACTION": 0.15, # Not used in the app
#     "TEST_FRACTION": 0.15, # Not used in the app
#     "RANDOM_SEED": 1337,
#     "EPOCHS_AE": 20, # Not used in the app
#     "EPOCHS_CNN": 20, # Not used in the app
#     "BATCH": 128, # Not used in the app
#     "NUM_LEADS": 2, # Adjusted for 2-lead processing
#     "PAD_VALUE": 0.0,
#     "LEAD_PRIORITY": ["MLII","V5","V1","V2","V3","V4","V6","I","II","III","aVR","aVL","aVF"]
# }
# 
# # --- Initial Setup ---
# os.makedirs(CONFIG["WORKDIR"], exist_ok=True)
# random.seed(CONFIG["RANDOM_SEED"])
# np.random.seed(CONFIG["RANDOM_SEED"])
# tf.random.set_seed(CONFIG["RANDOM_SEED"])
# 
# # Define DATA_DIR
# DATA_DIR = f'{CONFIG["WORKDIR"]}/mitdb'
# os.makedirs(DATA_DIR, exist_ok=True)
# 
# # Define the path for models
# MODEL_DIR = "./models"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# # Safer bandpass()
# def bandpass(sig, fs, lo, hi, order=4):
#     nyq = fs/2.0
#     lo_n = max(1e-6, float(lo)/nyq)
#     hi_n = min(0.999, float(hi)/nyq)
#     if not (0 < lo_n < hi_n < 1):
#         # fallback: clamp to a safe band
#         lo_n, hi_n = 0.004, 0.499
#     b, a = butter(order, [lo_n, hi_n], btype='band')
#     return filtfilt(b, a, sig, method="gust")
# 
# NORMAL_SYMS=set(['N','L','R','e','j']); PVC_SYMS=set(['V','E'])
# APB_SYMS=set(['A','a','J','S']); FUSION_SYMS=set(['F'])
# def map_symbol(s, mode="binary"):
#     if mode=="binary": return 0 if s in NORMAL_SYMS else 1
#     if s in NORMAL_SYMS: return 0
#     if s in PVC_SYMS: return 1
#     if s in APB_SYMS: return 2
#     if s in FUSION_SYMS: return 3
#     return 4
# 
# def pick_lead_indices(sig_names, wanted, k):
#     idx=[]
#     for name in wanted:
#         if name in sig_names and len(idx)<k: idx.append(sig_names.index(name))
#     for i,_ in enumerate(sig_names):
#         if len(idx)>=k: break
#         if i not in idx: idx.append(i)
#     return idx[:k]
# 
# def preprocess_single_lead(x, fs_src, fs_tgt, bp):
#     up, dn = fs_tgt, int(fs_src)
#     g=gcd(up,dn); up//=g; dn//=g
#     x_rs=resample_poly(x, up, dn)
#     x_bp=bandpass(x_rs, fs_tgt, bp[0], bp[1])
#     return (x_bp-np.mean(x_bp))/(np.std(x_bp)+1e-8)
# 
# def load_record_segments_multi(path_noext, fs_tgt=CONFIG["TARGET_FS"], win_sec=CONFIG["WIN_SEC"], mode="binary"):
#     rec=wfdb.rdrecord(path_noext); fs_src=rec.fs
#     sig=rec.p_signal.astype(np.float64)
#     sig_names=list(rec.sig_name)
#     K=CONFIG["NUM_LEADS"]
#     use_idx=pick_lead_indices(sig_names, CONFIG["LEAD_PRIORITY"], min(K,sig.shape[1]))
#     chosen=[preprocess_single_lead(sig[:,i], fs_src, fs_tgt, CONFIG["BANDPASS"]) for i in use_idx]
#     Xstack=np.stack(chosen,axis=0)
#     if len(use_idx)<K:
#         pad=np.full((K-len(use_idx),Xstack.shape[1]), CONFIG["PAD_VALUE"])
#         Xstack=np.concatenate([Xstack,pad],axis=0)
#     Xstack=Xstack.T  # (T,K)
# 
#     try:
#         ann=wfdb.rdann(path_noext,'atr')
#         annsamp=(ann.sample*(fs_tgt/fs_src)).astype(int)
#         anntype=ann.symbol
#     except:
#         annsamp=processing.gqrs_detect(sig=Xstack[:,0],fs=fs_tgt)
#         anntype=['N']*len(annsamp)
# 
#     half=int(win_sec*fs_tgt/2); Ttot=Xstack.shape[0]
#     segment_len = 2 * half
#     Xs,ys=[],[]
#     for s,sy in zip(annsamp,anntype):
#         l = s - half
#         r = s + half
# 
#         pad_l = max(0, -l)
#         pad_r = max(0, r - Ttot)
# 
#         l_padded = l + pad_l
#         r_padded = r + pad_r
# 
#         if pad_l > 0 or pad_r > 0:
#             padded_Xstack = np.pad(Xstack, ((pad_l, pad_r), (0, 0)), mode='constant', constant_values=CONFIG["PAD_VALUE"])
#         else:
#             padded_Xstack = Xstack
# 
#         seg = padded_Xstack[l_padded:r_padded, :]
# 
#         if seg.shape[0] == segment_len:
#             Xs.append(seg.astype(np.float32)); ys.append(map_symbol(sy,mode))
#     if not Xs: return np.empty((0,segment_len,K)), np.empty((0,))
#     return np.stack(Xs), np.asarray(ys)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# import pandas as pd # Import pandas for correlation calculation
# 
# def load_record_segments_multi(path_noext, fs_tgt=CONFIG["TARGET_FS"], win_sec=CONFIG["WIN_SEC"], mode="binary"):
#     rec=wfdb.rdrecord(path_noext); fs_src=rec.fs
#     sig=rec.p_signal.astype(np.float64)
#     sig_names=list(rec.sig_name)
#     K=CONFIG["NUM_LEADS"] # Should be 2 based on the CONFIG update in the previous step
# 
#     # --- Lead Selection Logic ---
#     if sig.shape[1] > 1:
#         # Calculate pairwise correlation
#         corr_matrix = np.corrcoef(sig.T)
#         # Get absolute values and set diagonal to 1 (correlation with itself is 1)
#         abs_corr_matrix = np.abs(corr_matrix)
#         np.fill_diagonal(abs_corr_matrix, 1.0)
# 
#         # Find the indices of the two leads with the lowest absolute correlation
#         # Initialize with a high value
#         min_corr = 2.0
#         use_idx = []
# 
#         for i in range(sig.shape[1]):
#             for j in range(i + 1, sig.shape[1]):
#                 if abs_corr_matrix[i, j] < min_corr:
#                     min_corr = abs_corr_matrix[i, j]
#                     use_idx = [i, j]
# 
#         # If only one lead is available, use that lead
#         if sig.shape[1] == 1:
#             use_idx = [0]
# 
#     else:
#         # If only one lead is available or no leads, use the available leads (which is 0 or 1)
#          use_idx = list(range(sig.shape[1]))
# 
#     # Ensure we don't select more leads than available or configured
#     use_idx = use_idx[:min(len(use_idx), K)]
#     # --- End Lead Selection Logic ---
# 
# 
#     chosen=[preprocess_single_lead(sig[:,i], fs_src, fs_tgt, CONFIG["BANDPASS"]) for i in use_idx]
#     Xstack=np.stack(chosen,axis=0)
#     if len(use_idx)<K:
#         pad=np.full((K-len(use_idx),Xstack.shape[1]), CONFIG["PAD_VALUE"])
#         Xstack=np.concatenate([Xstack,pad],axis=0)
#     Xstack=Xstack.T  # (T,K)
# 
#     try:
#         ann=wfdb.rdann(path_noext,'atr')
#         annsamp=(ann.sample*(fs_tgt/fs_src)).astype(int)
#         anntype=ann.symbol
#     except:
#         # If annotation fails, use gqrs_detect on the first selected lead for R-peak detection
#         if Xstack.shape[1] > 0:
#             annsamp=processing.gqrs_detect(sig=Xstack[:,0],fs=fs_tgt)
#             anntype=['N']*len(annsamp)
#         else:
#             # Handle case where no leads are available
#             return np.empty((0,int(CONFIG["WIN_SEC"]*CONFIG["TARGET_FS"]),K)), np.empty((0,))
# 
# 
#     half=int(win_sec*fs_tgt/2); Ttot=Xstack.shape[0]
#     segment_len = 2 * half
#     Xs,ys=[],[]
#     for s,sy in zip(annsamp,anntype):
#         l = s - half
#         r = s + half
# 
#         pad_l = max(0, -l)
#         pad_r = max(0, r - Ttot)
# 
#         l_padded = l + pad_l
#         r_padded = r + pad_r
# 
#         if pad_l > 0 or pad_r > 0:
#             padded_Xstack = np.pad(Xstack, ((pad_l, pad_r), (0, 0)), mode='constant', constant_values=CONFIG["PAD_VALUE"])
#         else:
#             padded_Xstack = Xstack
# 
#         seg = padded_Xstack[l_padded:r_padded, :]
# 
#         if seg.shape[0] == segment_len:
#             Xs.append(seg.astype(np.float32)); ys.append(map_symbol(sy,mode))
#     if not Xs: return np.empty((0,segment_len,K)), np.empty((0,))
#     return np.stack(Xs), np.asarray(ys)
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# # --- Model and Threshold Loading ---
# MODEL_DIR = "./models"
# AE_MODEL_PATH = os.path.join(MODEL_DIR, "autoencoder.keras")
# CNN_MODEL_PATH = os.path.join(MODEL_DIR, "cnn_2lead.keras")
# AE_THRESHOLD_PATH = os.path.join(MODEL_DIR, "ae_threshold.npy")
# 
# ae_model = None
# clf_model = None
# ae_threshold = None
# 
# try:
#     ae_model = keras.models.load_model(AE_MODEL_PATH)
#     clf_model = keras.models.load_model(CNN_MODEL_PATH)
#     ae_threshold = np.load(AE_THRESHOLD_PATH)
#     st.success("Models and threshold loaded successfully!")
# except FileNotFoundError:
#     st.error(f"Model or threshold file not found. Please ensure the '{MODEL_DIR}' directory exists and contains:")
#     st.error(f"- {os.path.basename(AE_MODEL_PATH)}")
#     st.error(f"- {os.path.basename(CNN_MODEL_PATH)}")
#     st.error(f"- {os.path.basename(AE_THRESHOLD_PATH)}")
# except Exception as e:
#     st.error(f"An error occurred while loading models or threshold: {e}")
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# # --- Analysis Function ---
# def analyze_record(record_id):
#     """
#     Downloads, preprocesses, and analyzes a single PhysioNet record
#     for anomaly detection and classification.
# 
#     Args:
#         record_id (str): The ID of the PhysioNet record to analyze.
# 
#     Returns:
#         tuple: A tuple containing the overall anomaly rate (float) and
#                the overall abnormal beat rate (float), or (None, None)
#                if no segments were found.
#     """
#     if ae_model is None or clf_model is None or ae_threshold is None:
#         st.error("Models or threshold are not loaded.")
#         return None, None
# 
#     # Download the record if not already present
#     record_path_noext = os.path.join(DATA_DIR, record_id)
#     if not os.path.exists(f"{record_path_noext}.hea"):
#         st.info(f"Downloading record {record_id}...")
#         try:
#             wfdb.dl_records([record_id], DATA_DIR)
#             st.success(f"Downloaded record {record_id}.")
#         except Exception as e:
#             st.error(f"Error downloading record {record_id}: {e}")
#             return None, None
# 
#     # Load and preprocess segments
#     st.info(f"Processing segments for record {record_id}...")
#     Xr, yr = load_record_segments_multi(record_path_noext,
#                                          fs_tgt=CONFIG["TARGET_FS"],
#                                          win_sec=CONFIG["WIN_SEC"],
#                                          mode="binary")
# 
#     if Xr.size == 0:
#         st.warning(f"No segments found for record {record_id}.")
#         return 0.0, 0.0 # Return 0 rates if no segments
# 
#     st.info(f"Found {len(Xr)} segments for record {record_id}.")
# 
#     # Perform anomaly detection using AE
#     st.info("Performing anomaly detection...")
#     rec_recon = ae_model.predict(Xr, verbose=0)
#     rec_err = np.mean((rec_recon - Xr)**2, axis=(1, 2))
#     ae_pred = (rec_err > ae_threshold).astype(int)
# 
#     # Perform classification using CNN
#     st.info("Performing classification...")
#     log = clf_model.predict(Xr, verbose=0)
#     c_pred = np.argmax(log, axis=1)
# 
#     # Calculate overall rates
#     overall_anomaly_rate = np.mean(ae_pred)
#     overall_abnormal_beat_rate = np.mean(c_pred == 1) # Assuming 1 is the abnormal class in binary mode
# 
#     return overall_anomaly_rate, overall_abnormal_beat_rate
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# # --- Streamlit UI ---
# st.title("ECG Anomaly Detection and Classification")
# 
# record_id = st.text_input("Enter PhysioNet Record ID (e.g., 100)", "100")
# 
# if st.button("Analyze Record"):
#     if record_id:
#         overall_anomaly_rate, overall_abnormal_beat_rate = analyze_record(record_id)
# 
#         if overall_anomaly_rate is not None and overall_abnormal_beat_rate is not None:
#             st.info(f"Overall Anomaly Rate (AE): {overall_anomaly_rate:.4f}")
#             st.info(f"Overall Abnormal Beat Rate (CNN): {overall_abnormal_beat_rate:.4f}")
# 
#             # Optional: Add sections for sample plots here if implemented in analyze_record or separately
#             # For example:
#             # st.subheader("Sample Segment Predictions")
#             # (Code to display plots)
# 
#         elif overall_anomaly_rate == 0.0 and overall_abnormal_beat_rate == 0.0:
#             st.warning(f"No segments found for record {record_id}. Cannot perform analysis.")
#         else:
#              st.error(f"Analysis failed for record {record_id}.")
#     else:
#         st.warning("Please enter a record ID.")
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# # --- How to Run the Application ---
# # 1. Save this code: Save the content of this cell as `app.py`.
# # 2. Create a 'models' directory: In the same directory where you saved `app.py`, create a new folder named `models`.
# # 3. Place model files: Put your trained Keras Autoencoder model (saved as `autoencoder.keras`),
# #    your trained Keras CNN model (saved as `cnn_2lead.keras`), and your calculated AE threshold (saved as `ae_threshold.npy`)
# #    inside the `models` directory.
# # 4. Install dependencies: Make sure you have the necessary libraries installed by running:
# #    pip install streamlit wfdb numpy scipy scikit-learn tensorflow matplotlib tqdm
# # 5. Run the application: Open your terminal or command prompt, navigate to the directory where you saved `app.py`,
# #    and run the following command:
# #    streamlit run app.py
# #    This will start the Streamlit server and open the application in your web browser.