# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12j8mgir2-7QwjzCxzdOCWlygdXjC11Wa
"""

writefile app.py
# app.py â€” Streamlit PhysioNet single-record inference (AE + CNN, multi-lead pipeline)

import os
import time
import random
import math
import numpy as np
import streamlit as st
import matplotlib.pyplot as plt

# libs used in training
import wfdb
from scipy.signal import butter, filtfilt, resample_poly
from tensorflow import keras

# -------------------------
# CONFIG (edit if needed)
# -------------------------
CONFIG = {
    "WORKDIR": "./physionet_data",
    "MODELS_DIR": "./models",                     # put autoencoder.keras and cnn_2lead.keras here
    "AUTOENC_PATH": "./models/autoencoder.keras",
    "CLF_PATH": "./models/cnn_2lead.keras",
    "AE_THRESHOLD_PATH": "./models/ae_threshold.npy", # Path to save/load AE threshold

    "TARGET_FS": 250, # Match the sampling frequency used during training
    "BANDPASS": (0.5, 30.0), # Match the bandpass filter used during training
    "WIN_SEC": 2.0, # Match the window size used during training
    "FRAME_LEN": int(2.0 * 250), # Window size in samples (WIN_SEC * TARGET_FS)
    # Note: Frame length and stride are less relevant here as we are segmenting based on annotations/R-peaks
    # However, keep them consistent with training if possible or adapt segmentation logic.
    # Stride is not used in the original load_record_segments_multi, segmenting is annotation-based.

    "NUM_LEADS": 3, # Match the number of leads used during training
    "PAD_VALUE": 0.0, # Match the padding value used during training
    "LEAD_PRIORITY": ["MLII","V5","V1","V2","V3","V4","V6","I","II","III","aVR","aVL","aVF"], # Match lead priority

    "SEED": 42,
    "EPS_STD": 1e-8,
}

random.seed(CONFIG["SEED"])
np.random.seed(CONFIG["SEED"])

# Ensure work directory and model directory exist
os.makedirs(CONFIG["WORKDIR"], exist_ok=True)
os.makedirs(CONFIG["MODELS_DIR"], exist_ok=True)
# -------------------------
# Data Loading and Preprocessing Functions (Adapted from Notebook)
# -------------------------

# Safer bandpass() - adapted from notebook fix
def bandpass(sig, fs, lo, hi, order=4):
    nyq = fs/2.0
    lo_n = max(1e-6, float(lo)/nyq)
    hi_n = min(0.999, float(hi)/nyq)
    if not (0 < lo_n < hi_n < 1):
        # fallback: clamp to a safe band
        lo_n, hi_n = 0.004, 0.499
    b, a = butter(order, [lo_n, hi_n], btype='band')
    return filtfilt(b, a, sig, method="gust")

# Preprocessing for a single lead - adapted from notebook fix
def preprocess_single_lead(x, fs_src, fs_tgt, bp):
    """
    Resample and bandpass filter a single lead signal.

    Args:
        x (np.ndarray): Input signal for a single lead.
        fs_src (int): Source sampling frequency.
        fs_tgt (int): Target sampling frequency.
        bp (tuple): Bandpass filter frequencies (lo, hi).

    Returns:
        np.ndarray: Processed signal.
    """
    # Ensure scipys resample_poly is available
    # In app.py, we assume SciPy is installed as a dependency
    # if not SCIPY_OK:
    #     st.error("SciPy not available. Cannot resample signal. Please ensure SciPy is installed.")
    #     return x # Return original signal if resampling is not possible

    # Resample
    from math import gcd # Import gcd here for local scope
    up, dn = fs_tgt, int(fs_src)
    g = gcd(up, dn)
    up //= g
    dn //= g
    x_rs = resample_poly(x, up, dn)

    # Bandpass filter
    x_bp = bandpass(x_rs, fs_tgt, bp[0], bp[1])

    # Standardize (handle potential zero std)
    mean_x = np.mean(x_bp)
    std_x = np.std(x_bp)
    if std_x < CONFIG["EPS_STD"]:
        return x_bp - mean_x # Just remove mean if std is too small
    return (x_bp - mean_x) / std_x

# Helper to pick lead indices based on priority
def pick_lead_indices(sig_names, wanted, k):
    idx = []
    for name in wanted:
        if name in sig_names and len(idx) < k:
            idx.append(sig_names.index(name))
    # Fill up remaining indices if needed from available leads
    for i, _ in enumerate(sig_names):
        if len(idx) >= k:
            break
        if i not in idx:
            idx.append(i)
    return idx[:k]

# Load record segments - adapted from notebook, including padding fix
def load_record_segments_multi(path_noext, fs_tgt=250, win_sec=2.0, mode="binary"):
    """
    Loads a PhysioNet record, preprocesses selected leads, and segments it
    around annotations or detected R-peaks. Handles padding.

    Args:
        path_noext (str): Path to the record file without extension.
        fs_tgt (int): Target sampling frequency.
        win_sec (float): Window size in seconds.
        mode (str): Classification mode (used for mapping symbols).

    Returns:
        tuple: (segments, labels) - numpy arrays of shape (N, segment_len, num_leads)
               and (N,), where N is the number of segments. Returns empty arrays
               if no segments are found.
    """
    try:
        rec = wfdb.rdrecord(path_noext)
        fs_src = rec.fs
        sig = rec.p_signal.astype(np.float64)
        sig_names = list(rec.sig_name)
    except Exception as e:
        st.error(f"Error reading record {os.path.basename(path_noext)}: {e}")
        return np.empty((0, int(win_sec * fs_tgt), CONFIG["NUM_LEADS"])), np.empty((0,))

    K = CONFIG["NUM_LEADS"]
    # Select leads based on priority, handling cases with fewer leads
    use_idx = pick_lead_indices(sig_names, CONFIG["LEAD_PRIORITY"], min(K, sig.shape[1]))

    if not use_idx:
        st.warning(f"Could not find any leads from priority list in record {os.path.basename(path_noext)}.")
        return np.empty((0, int(win_sec * fs_tgt), K)), np.empty((0,))

    # Preprocess chosen leads
    chosen = [preprocess_single_lead(sig[:, i], fs_src, fs_tgt, CONFIG["BANDPASS"]) for i in use_idx]

    # Stack preprocessed leads
    Xstack = np.stack(chosen, axis=0)

    # Pad with zeros if fewer than K leads were available
    if len(use_idx) < K:
        pad_shape = (K - len(use_idx), Xstack.shape[1])
        pad = np.full(pad_shape, CONFIG["PAD_VALUE"], dtype=Xstack.dtype)
        Xstack = np.concatenate([Xstack, pad], axis=0)

    Xstack = Xstack.T  # Shape becomes (T, K)

    # Get annotations or detect R-peaks
    try:
        ann = wfdb.rdann(path_noext, 'atr')
        # Adjust annotation samples to target frequency
        annsamp = (ann.sample * (fs_tgt / fs_src)).astype(int)
        anntype = ann.symbol
        # st.write(f"Record {os.path.basename(path_noext)}: Loaded {len(annsamp)} annotations.") # Optional: verbose logging
    except Exception as e:
        # st.warning(f"Could not load annotations for record {os.path.basename(path_noext)}. Attempting R-peak detection. Error: {e}") # Optional: verbose logging
        try:
            # Use gqrs_detect on the first available lead for R-peak detection
            annsamp = processing.gqrs_detect(sig=Xstack[:, 0], fs=fs_tgt)
            anntype = ['N'] * len(annsamp) # Assume all detected peaks are normal
            # st.write(f"Record {os.path.basename(path_noext)}: Detected {len(annsamp)} R-peaks.") # Optional: verbose logging
        except Exception as e_gqrs:
            st.error(f"Failed to detect R-peaks for record {os.path.basename(path_noext)}. Error: {e_gqrs}")
            return np.empty((0, int(win_sec * fs_tgt), K)), np.empty((0,))


    half = int(win_sec * fs_tgt / 2)
    segment_len = 2 * half
    Ttot = Xstack.shape[0]
    Xs, ys = [], []
    # Extract segments around annotation/R-peak samples
    for s, sy in zip(annsamp, anntype):
        # Calculate the start and end indices for the segment
        l = s - half
        r = s + half

        # Calculate required padding
        pad_l = max(0, -l)
        pad_r = max(0, r - Ttot)

        # Adjust start and end indices based on padding
        l_padded = l + pad_l
        r_padded = r + pad_r

        # Pad the signal array if necessary
        if pad_l > 0 or pad_r > 0:
            padded_Xstack = np.pad(Xstack, ((pad_l, pad_r), (0, 0)), mode='constant', constant_values=CONFIG["PAD_VALUE"])
        else:
            padded_Xstack = Xstack

        # Extract the segment from the padded array
        # Ensure segment length is correct after padding adjustment
        seg = padded_Xstack[l_padded : l_padded + segment_len, :]


        # Check if the extracted segment has the correct length (should be guaranteed by padding logic now)
        if seg.shape[0] == segment_len:
             # Check for NaN/Inf in the segment after processing
            if np.any(np.isnan(seg)) or np.any(np.isinf(seg)):
                # Optionally handle or skip segments with invalid values
                # st.warning(f"Segment at sample {s} contains NaN or Inf values. Skipping.") # Optional: verbose logging
                continue
            Xs.append(seg.astype(np.float32))
            # Map symbol to label (assuming binary mode 0=Normal, 1=Abnormal)
            if mode == "binary":
                 # Need map_symbol function for binary mapping
                 NORMAL_SYMS=set(['N','L','R','e','j']); PVC_SYMS=set(['V','E']) # Define here or import
                 ys.append(0 if sy in NORMAL_SYMS else 1)
            else:
                 # Handle other modes if needed
                 ys.append(-1) # Placeholder for other modes


    # st.write(f"Record {os.path.basename(path_noext)}: Found {len(Xs)} valid segments.") # Optional: verbose logging

    if not Xs:
        return np.empty((0, segment_len, K)), np.empty((0,))

    # Note: For prediction in app, we might not need the ground truth labels 'ys'
    # but we return them for consistency if available (from annotations)
    return np.stack(Xs), np.asarray(ys)


# Helper function for mapping symbols (copied from notebook)
NORMAL_SYMS=set(['N','L','R','e','j']); PVC_SYMS=set(['V','E'])
APB_SYMS=set(['A','a','J','S']); FUSION_SYMS=set(['F'])
def map_symbol(s, mode="binary"):
    if mode=="binary": return 0 if s in NORMAL_SYMS else 1
    if s in NORMAL_SYMS: return 0
    if s in PVC_SYMS: return 1
    if s in APB_SYMS: return 2
    if s in FUSION_SYMS: return 3
    return 4

# Helper to check if a channel is good (copied from notebook)
def is_channel_good(x, eps=1e-8):
    return np.all(np.isfinite(x)) and (np.std(x) > eps)
    # Helper to find the pair with lowest correlation (copied from notebook)
def pair_with_lowest_corr(leads_2d):
    # leads_2d: (L, T)
    L, T = leads_2d.shape
    best = (0, 1, 2.0) # Initialize with a high correlation value
    for i in range(L):
        for j in range(i+1, L):
            xi, xj = leads_2d[i], leads_2d[j]
            # Ensure both channels are 'good' before calculating correlation
            if is_channel_good(xi) and is_channel_good(xj):
                r = np.corrcoef(xi, xj)[0, 1]
                a = abs(r) if np.isfinite(r) else 2.0 # Use absolute correlation, handle NaN/Inf from corrcoef
            else:
                a = 2.0 # Assign high correlation if channels are not good
            if a < best[2]:
                best = (i, j, a)
    return best # Returns indices within the provided leads_2d, not original signal indices


# Function to select the best 2 leads based on lowest correlation
# This function is not strictly needed if load_record_segments_multi is adapted
# to do lead selection based on priority and CONFIG["NUM_LEADS"].
# If you want a specific "best 2 leads" *model* vs a multi-lead *model* with padding,
# you would need to adjust which model you load and how data is prepared.
# Let's stick to the multi-lead approach with padding as used in training for this app.
# If you specifically need a "best 2 leads" selection before padding for a 2-lead model,
# the load_record_segments_multi function would need significant modification or a new function created.
# For consistency with the multi-lead training notebook, the load_record_segments_multi
# handles lead selection based on CONFIG["LEADS_PRIORITY"] and CONFIG["NUM_LEADS"] with padding.


# -------------------------
# Streamlit App
# -------------------------

st.title("ECG Anomaly Detection and Classification (AE + CNN)")
st.write("Test PhysioNet records using trained Autoencoder and CNN models.")

# --- Load Models ---
# Use Streamlit caching to avoid reloading models on every interaction
@st.cache_resource
def load_models(ae_path, clf_path):
    ae_model = None
    clf_model = None

    if not os.path.exists(ae_path):
         st.warning(f"Autoencoder model file not found at {ae_path}. Please train and save the model.")
    else:
        try:
            ae_model = keras.models.load_model(ae_path)
            st.success(f"Autoencoder model loaded successfully from {ae_path}")
        except Exception as e:
            st.error(f"Error loading Autoencoder model from {ae_path}: {e}")

    if not os.path.exists(clf_path):
         st.warning(f"CNN classifier model file not found at {clf_path}. Please train and save the model.")
    else:
        try:
            clf_model = keras.models.load_model(clf_path)
            st.success(f"CNN classifier model loaded successfully from {clf_path}")
        except Exception as e:
            st.error(f"Error loading CNN classifier model from {clf_path}: {e}")


    return ae_model, clf_model

# Load the models when the app starts
ae_model, clf_model = load_models(CONFIG["AUTOENC_PATH"], CONFIG["CLF_PATH"])

# --- Load AE Threshold ---
# Use Streamlit caching for the threshold
@st.cache_data
def load_ae_threshold(threshold_path):
    if not os.path.exists(threshold_path):
         st.warning(f"AE threshold file not found at {threshold_path}. Anomaly detection will not work.")
         st.warning("Please run the notebook to train the autoencoder and save the threshold.")
         return None
    else:
        try:
            threshold = np.load(threshold_path)
            st.success(f"AE threshold loaded successfully from {threshold_path}")
            return float(threshold)
        except Exception as e:
            st.error(f"Error loading AE threshold from {threshold_path}: {e}")
            return None

ae_threshold = load_ae_threshold(CONFIG["AE_THRESHOLD_PATH"])

# --- User Input ---
st.sidebar.header("Settings")
record_id_input = st.sidebar.text_input("Enter PhysioNet Record ID (e.g., 100)", "100")
# You can add more settings here, e.g., number of segments to plot

# --- Prediction ---
if st.button("Analyze Record"):
    if ae_model is None or clf_model is None:
        st.error("Models are not loaded. Please ensure the model files exist in the 'models' directory and were loaded successfully.")
    elif ae_threshold is None:
         st.error("AE threshold is not loaded. Anomaly detection cannot be performed.")
    else:
        st.info(f"Analyzing record: {record_id_input}...")
        start_time = time.time()

        DATA_DIR = f'{CONFIG["WORKDIR"]}/mitdb'
        record_path_noext = os.path.join(DATA_DIR, record_id_input)

        # 1. Download record if not exists
        # Need to handle potential errors during download
        try:
            if not os.path.exists(f"{record_path_noext}.hea"):
                st.write(f"Record {record_id_input} not found locally. Attempting download from PhysioNet...")
                # Removed raise_overwrite_error=False from dl_database call
                wfdb.dl_database('mitdb', dl_dir=DATA_DIR, records=[record_id_input])
                st.success(f"Downloaded record {record_id_input}.")
            else:
                 st.write(f"Record {record_id_input} found locally.")
        except Exception as e:
            st.error(f"Error downloading record {record_id_input} from PhysioNet: {e}")
            st.stop() # Stop execution if download fails


        # 2. Load and preprocess segments
        # load_record_segments_multi will handle lead selection based on CONFIG["NUM_LEADS"]
        # and padding for records with fewer leads, matching the training setup.
        # If you have a specific 'best 2 leads' model, you'd need to adapt this loading.
        # For consistency with the notebook's training (using CONFIG["NUM_LEADS"] and padding),
        # we use the multi-lead loading here.

        X_record, y_record_true = load_record_segments_multi(record_path_noext,
                                                              fs_tgt=CONFIG["TARGET_FS"],
                                                              win_sec=CONFIG["WIN_SEC"],
                                                              mode="binary") # Assuming binary mode for prediction


        if X_record.size == 0:
            st.warning(f"No valid segments found for record {record_id_input}.")
        else:
            # 3. Perform Predictions
            st.write(f"Found {len(X_record)} segments. Performing predictions...")
             # Autoencoder prediction (anomaly detection)
            # Ensure ae_model is not None before predicting
            if ae_model:
                ae_rec = ae_model.predict(X_record, verbose=0)
                ae_err = np.mean((ae_rec - X_record)**2, axis=(1, 2))

                # Anomaly prediction based on threshold (ensure threshold is loaded)
                if ae_threshold is not None:
                     ae_pred_anomalies = (ae_err > ae_threshold).astype(int)
                     ae_anomaly_rate = ae_pred_anomalies.mean()
                else:
                     ae_anomaly_rate = None
                     st.warning("AE threshold not loaded. Skipping AE anomaly detection.")
            else:
                 ae_anomaly_rate = None
                 st.warning("AE model not loaded. Skipping AE anomaly detection.")


            # CNN classifier prediction
            # Ensure clf_model is not None before predicting
            if clf_model:
                cnn_log = clf_model.predict(X_record, verbose=0)
                cnn_pred_classes = np.argmax(cnn_log, axis=1)
                cnn_abnormal_rate = (cnn_pred_classes == 1).mean() # Assuming 1 is the abnormal class
            else:
                 cnn_abnormal_rate = None
                 st.warning("CNN model not loaded. Skipping CNN classification.")


            end_time = time.time()

            # 4. Display Results
            st.subheader("Analysis Results")
            st.write(f"Record ID: **{record_id_input}**")
            st.write(f"Total Segments Analyzed: **{len(X_record)}**")
            st.write(f"Analysis Time: **{end_time - start_time:.2f} seconds**")

            st.subheader("Autoencoder Anomaly Detection")
            if ae_anomaly_rate is not None:
                 st.write(f"Proportion of segments detected as anomalies (AE Error > {ae_threshold:.4f}): **{ae_anomaly_rate:.3f}**")
            else:
                 st.write("AE Anomaly Detection skipped due to missing model or threshold.")


            st.subheader("CNN Classification")
            if cnn_abnormal_rate is not None:
                 st.write(f"Proportion of segments classified as abnormal: **{cnn_abnormal_rate:.3f}**")
            else:
                 st.write("CNN Classification skipped due to missing model.")


            # Display sample segments with predictions
            st.subheader("Sample Segment Predictions")
            num_plots = min(4, len(X_record))
            if num_plots > 0 and ae_model and clf_model and ae_threshold is not None:
                sample_indices = np.random.choice(len(X_record), num_plots, replace=False)
                # Use FRAME_LEN which is segment_len calculated as 2*half
                segment_len = int(CONFIG["WIN_SEC"] * CONFIG["TARGET_FS"])
                t = np.linspace(-CONFIG["WIN_SEC"]/2, CONFIG["WIN_SEC"]/2, segment_len)
                for i, j in enumerate(sample_indices):
                    fig, ax = plt.subplots(figsize=(10, 3))
                    for ch in range(X_record.shape[2]):
                        # Offset leads for visibility
                        ax.plot(t, X_record[j, :, ch] + 3 * ch)
                    ax.set_title(f"Segment {j+1} | AE Error={ae_err[j]:.4f} (Anomaly: {ae_pred_anomalies[j]}) | CNN Pred Class: {cnn_pred_classes[j]}")
                    ax.set_xlabel("Time (s)")
                    ax.set_ylabel("Amplitude (Offset for Leads)")
                    st.pyplot(fig)
                    plt.close(fig) # Close the figure to free memory
            else:
                if num_plots == 0:
                    st.write("No segments to plot.")
                else:
                    st.write("Skipping sample segment plots due to missing models or threshold.")


                    # --- Instructions for Running ---
st.sidebar.subheader("How to Run")
st.sidebar.write("1. Ensure you have `streamlit`, `wfdb`, `numpy`, `scipy`, and `tensorflow` installed (`pip install streamlit wfdb numpy scipy tensorflow`).")
st.sidebar.write("2. Save the trained models (`autoencoder.keras`, `cnn_2lead.keras`) and the AE threshold (`ae_threshold.npy`) into a folder named `models` in the same directory as this script.")
st.sidebar.write("3. Save this code as `app.py`.")
st.sidebar.write("4. Open your terminal or command prompt.")
st.sidebar.write(f"5. Navigate to the directory where you saved `app.py` and the `models` folder.")
st.sidebar.write("6. Run the command: `streamlit run app.py`")
st.sidebar.write("7. The app will open in your web browser.")

st.sidebar.subheader("Note on Trained Models")
st.sidebar.write(f"This app assumes the models were trained with {CONFIG['NUM_LEADS']} leads (padded if necessary), a target frequency of {CONFIG['TARGET_FS']} Hz, and a window size of {CONFIG['WIN_SEC']} seconds.")
st.sidebar.write("Ensure your saved models and threshold match these parameters.")